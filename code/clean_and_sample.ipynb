{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b89c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title: GENDER PREDICTION FROM NEW YORK CITYâ€™S BIKE-SHARING DATA: \n",
    "#        A MACHINE LEARNING AND DEEP LEARNING APPROACH\n",
    "\n",
    "# Author: Thorsten Picard\n",
    "# Programme: MSc. Data Science and Society\n",
    "# University: Tilburg University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37905f0",
   "metadata": {},
   "source": [
    "**READ ME**\n",
    "\n",
    "This file imports the raw monthly Citi Bike datasets from 2019, performs a few basic cleaning operations, and takes a sample from the cleaned data. The cleaning operations are the following:\n",
    "* Discard roundtrips (where station ID == end station ID)\n",
    "* Discard rows with a birth year that results in an age of 100 or larger\n",
    "* Discard duplicate rows\n",
    "* Discard rows containing NA values\n",
    "* Discard rows where gender is not 1 (male) or 2 (female)\n",
    "\n",
    "The changes these operations bring about are stored in a file called \"monthly_stats_cleaned.csv\". This file allows anyone to see by how many rows the monthly datasets have been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a764ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052012d6",
   "metadata": {},
   "source": [
    "## Clean and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that cleans the data\n",
    "\n",
    "def clean_data(directory, file):\n",
    "    \n",
    "    # import data\n",
    "    df = pd.read_csv(directory+file, header=0)\n",
    "    \n",
    "    # temporary variables\n",
    "    df['roundtrip'] = df['start station id'] == df['end station id']\n",
    "    df['age'] = (2019 - df['birth year']) >= 100\n",
    "    \n",
    "    # Get lengths\n",
    "    len_df   = len(df) # row count original dataset\n",
    "    len_NaN  = np.sum(df.isna().any(axis=1)) # NaNs\n",
    "    len_dupl = np.sum(df.duplicated()) # duplicates\n",
    "    len_M    = np.sum(df['gender']==1) # count males\n",
    "    len_F    = np.sum(df['gender']==2) # count females\n",
    "    len_O    = len_df - len_M - len_F # count other gender values\n",
    "    len_RT   = np.sum(df['roundtrip']) # count roundtrips\n",
    "    len_age_100  = np.sum(df['age']) # count age >= 100\n",
    "    \n",
    "    # drop rows with NA, drop duplicate rows\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # only include rows with gender == 1 (male) or 2 (female)\n",
    "    df = df[df['gender'].isin([1, 2])]\n",
    "    \n",
    "    # discard roundtrips, discard rows with higher than boundary\n",
    "    df = df[df['roundtrip'] == False]\n",
    "    df = df[df['age'] == False]\n",
    "    \n",
    "    # get length of filtered dataset\n",
    "    len_filt   = len(df) # row count filtered dataset\n",
    "    len_filt_M = np.sum(df['gender']==1) # count males\n",
    "    len_filt_F = np.sum(df['gender']==2) # count females\n",
    "    \n",
    "    # drop temporary columns\n",
    "    cols_to_drop = ['roundtrip', 'age']\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    name = int(file[4:6])\n",
    "    \n",
    "    # collect all counts \n",
    "    all_lengths = [name, \n",
    "                   len_df, len_NaN, len_dupl, len_M, len_F, len_O, len_RT, len_age_100, \n",
    "                   len_filt, len_filt_M, len_filt_F]\n",
    "    \n",
    "    return df, all_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ab010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS\n",
      "-----------\n",
      "201901 - done\n",
      "201902 - done\n",
      "201903 - done\n",
      "201904 - done\n",
      "201905 - done\n",
      "201906 - done\n",
      "201907 - done\n",
      "201908 - done\n",
      "201909 - done\n",
      "201910 - done\n",
      "201911 - done\n",
      "201912 - done\n",
      "-----------\n",
      "ALL DONE: 12 files have been exported. Execution time: -472.978590965271 seconds (-7.9 min).\n"
     ]
    }
   ],
   "source": [
    "# clean each monthly dataset\n",
    "\n",
    "# settings\n",
    "imp_dir = '../Data/'\n",
    "exp_dir = '../data-2019-filtered/'\n",
    "files_list = [f for f in listdir(imp_dir) if f.startswith('2019')]\n",
    "files_list.sort()\n",
    "\n",
    "# statistics about the number of discarded rows per month\n",
    "d = []\n",
    "\n",
    "# start measuring time\n",
    "startTime = time.time()\n",
    "\n",
    "print(\"PROGRESS\")\n",
    "print(\"-----------\")\n",
    "\n",
    "# conversion\n",
    "for file in files_list:\n",
    "    \n",
    "    name = file[:6]\n",
    "\n",
    "    # clean data and collect statistics about the number of discarded rows\n",
    "    df, all_lengths = clean_data(imp_dir, file)\n",
    "    \n",
    "    # save cleaned data\n",
    "    df.to_csv(exp_dir+str(name)+'.csv', header=True, index=False)\n",
    "    del df\n",
    "    \n",
    "    # store statistics about the number of discarded rows\n",
    "    d.append(all_lengths)\n",
    "    \n",
    "    print(name, \"- done\")\n",
    "\n",
    "# stop measuring time and report\n",
    "executionTime = time.time() - startTime\n",
    "eT_min = np.round(executionTime / 60, 1)\n",
    "\n",
    "print(\"-----------\")\n",
    "print(\"ALL DONE:\", len(files_list), \"files have been exported. Execution time:\", np.round(executionTime, 0), \n",
    "      \"seconds (\"+str(eT_min)+\" min).\")\n",
    "\n",
    "\n",
    "# statistics about the number of discarded rows for each month\n",
    "cols = ['month', \n",
    "        'original_tripc', 'NaNs', 'duplicates', 'original_M', 'original_F', 'original_O', \n",
    "        'roundtrips','age_from_100', \n",
    "        'cleaned_tripc', 'cleaned_M', 'cleaned_F']\n",
    "\n",
    "monthly_2019 = pd.DataFrame(np.array(d), columns=cols, dtype=\"int\")\n",
    "monthly_2019.to_csv('../summaries/submission_2/monthly_stats_cleaned.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf1f44",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32ad38",
   "metadata": {},
   "source": [
    "Given the sheer volume of data (+ 18 million rows), samples will be subtracted from the 2019 dataset. A balanced and an unbalanced sample in terms of the target variable (gender) will be taken from the 2019 dataset for each random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1514535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    }
   ],
   "source": [
    "# obtain a random state, change size to increase the number of samples per sample type (bal, unb).  \n",
    "\n",
    "np.random.seed(6)\n",
    "random_states = np.random.randint(low = 0, high = 100, size = 1)\n",
    "\n",
    "print(random_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1998b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS\n",
      "-----------\n",
      "Sample with random state 10 - done.\n",
      "-----------\n",
      "ALL DONE - 2 samples have been exported. Execution time: 133.0 seconds (2.21 min).\n"
     ]
    }
   ],
   "source": [
    "# ------- FILE SETTINGS -------\n",
    "# files\n",
    "imp_dir = '../data-2019-filtered/'\n",
    "exp_dir = '../samples/original/'\n",
    "files_list = listdir(imp_dir)\n",
    "files_list.sort()\n",
    "\n",
    "\n",
    "# ------- SAMPLING -------\n",
    "\n",
    "# sample settings \n",
    "total_sample_size = 50000 # per month\n",
    "test_sample_size = int(.3 * total_sample_size)\n",
    "train_sample_size = int(total_sample_size - test_sample_size)\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "print(\"PROGRESS\")\n",
    "print(\"-----------\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(len(random_states)):\n",
    "    \n",
    "    rs = random_states[i]\n",
    "\n",
    "    # create one sample with all monthly samples\n",
    "    total_sample_unb = pd.DataFrame()\n",
    "    total_sample_bal = pd.DataFrame()\n",
    "    \n",
    "    # iterate over the original, monthly datasets\n",
    "    for file in files_list:\n",
    "\n",
    "        # import monthly dataset\n",
    "        path = imp_dir+file\n",
    "        df = pd.read_csv(path, header=0)\n",
    "        \n",
    "        # ---- UNBALANCED\n",
    "        sample_unb = df.sample(n=total_sample_size, random_state=rs)\n",
    "        total_sample_unb = pd.concat([total_sample_unb, sample_unb])\n",
    "        \n",
    "        # ---- BALANCED\n",
    "        # test set, unbalanced\n",
    "        test_data = df.sample(n=test_sample_size, random_state=rs)\n",
    "        test_data['set'] = ['test'] * test_data.shape[0]\n",
    "        test_indices = test_data.index.values # get indices of test set\n",
    "        df_new = df.drop(test_indices) # drop test set from original dataset\n",
    "        \n",
    "        # val set, unbalanced\n",
    "        val_data  = df_new.sample(n=int(.3*train_sample_size), random_state=rs)\n",
    "        val_data['set'] = ['val'] * val_data.shape[0]\n",
    "        val_indices = val_data.index.values # get indices of test set\n",
    "        df_new2 = df_new.drop(val_indices) # drop test set from original dataset        \n",
    "\n",
    "        # training set, balanced\n",
    "        male, female = 1, 2\n",
    "        df_new_M = df_new2[df_new2['gender']==male] # male set\n",
    "        df_new_F = df_new2[df_new2['gender']==female] # female set\n",
    "            \n",
    "        train_data = pd.DataFrame()\n",
    "        for gender_set in (df_new_M, df_new_F):\n",
    "            train_data = pd.concat([train_data, \n",
    "                                    gender_set.sample(n=int((.7*train_sample_size)/2), random_state=rs)])\n",
    "        train_data['set'] = ['train'] * train_data.shape[0]\n",
    "        \n",
    "        # check for duplicates\n",
    "        all_indices = np.concatenate((train_data.index, val_data.index, test_data.index), axis=None)\n",
    "        count_duplicates = np.sum(pd.Series(all_indices).duplicated())\n",
    "            \n",
    "        if count_duplicates == 0:\n",
    "            total_sample_bal = pd.concat([total_sample_bal, train_data, val_data, test_data])\n",
    "        else:\n",
    "            print(\"duplicates exist between train and test sets\")\n",
    "\n",
    "    # save total samples\n",
    "    total_sample_bal.to_csv(exp_dir+\"bal-sample-\"+str(i)+\".csv\", header=True, index=False)\n",
    "    count += 1\n",
    "    total_sample_unb.to_csv(exp_dir+\"unb-sample-\"+str(i)+\".csv\", header=True, index=False)\n",
    "    count += 1\n",
    "    \n",
    "    print(\"Sample with random state \"+str(rs)+\" - done.\")\n",
    "\n",
    "executionTime = time.time() - startTime\n",
    "eT_min = np.round(executionTime / 60, 2)\n",
    "\n",
    "print(\"-----------\")\n",
    "print(\"ALL DONE - \"+ str(count) +\" samples have been exported. Execution time:\", np.round(executionTime, 0), \n",
    "      \"seconds (\"+str(eT_min)+\" min).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
